\chapter{Localization Algorithm} \label{chap:pf}
This chapter presents the solution's implementation. As mentioned before, it is based on \acf{MCL}. The terminology used in this chapter builds upon the \acs{PF}'s introduction in section~\ref{sec:fund_pf}.

First the reasons for choosing \acs{MCL} instead of another algorithm are outlined. Afterwards, an overview of the system's setup is provided. Next the algorithm's \emph{motion model} is described, followed by a detailed insight in the solution's \emph{measurement model}. In the end, the algorithm's implementation is described, which builds upon the motion and measurement model.


\section{Design Decision} \label{sec:algo_decision}
This section outlines the reasons for choosing \acl{MCL} instead of another algorithm, as introduced in chapter~\ref{chap:fundamentals}.

As shown in chapter~\ref{chap:ibeacons} and mentioned by \citet{IEEE:survey_wireless_indoor_pos}, wireless signals are heavily influenced by obstacles and their environment. According to \citet{wang:wlan} and \citet{siddiqi:experiments_mcl_wifi}, location accuracy can be drastically improved by additionally using map information and motion tracking. Thus, the main reason for choosing \ac{MCL} is the fact, that \acl{PF} is the only algorithm, which has the ability to combine the \acs{RSS}-based measurements to the beacons with motion tracking by additionally using map information. Triangulation, the proximity method, and the scene analysis approach are not able to improve their location estimation by combining motion tracking with \ac{RSS}-based measurements. \acl{KF} is the only algorithm which is also capable of using motion besides \ac{PF}. But according to \citet{wang:wlan}, ``map information is impossible to be integrated for tracking by \acs{EKF}''.

The second reason for choosing \acs{PF} is its ability to solve the \emph{global localization problem}. Thus, the algorithm can start determining the user's location without knowing the user's initial position. Furthermore, the algorithm is able to recover form failure state, i.e.\ if the estimated location is completely wrong, e.g. due to short-term sensor failure, it is able to detect and to recover from that state.

The third reason is the algorithm's advantage of taking uncertainties into account. Besides \ac{PF}, \ac{KF} is the only mentioned algorithm which is also capable of modeling uncertainties. As mentioned before, \acs{PF} is a non-parametric filter with the advantage of expressing a location estimation in the form of a multi-modal posterior belief. Whereas \ac{KF} is a parametric filter which is fixed to normal distributed position estimations. Furthermore, the \ac{PF}'s multi-modal belief can be visually illustrated very well and the users can benefit from this as shown in figure~\ref{fig:pf_approx}.

The fourth reason are the, in chapter~\ref{chap:intro} defined, requirements. By using \acs{PF}, based on \acs{RSS}-based measurements, less pre-deployment effort is required. Furthermore, the required infrastructure is of little complexity compared to other solutions outlined in chapter~\ref{chap:fundamentals}, which reduces the maintenance effort and the initial and maintenance costs.

Besides the above mentioned reasons, \acl{PF} is an easy-to-implement algorithm. Furthermore, \ac{MCL} is a well-known and well-studied approach for landmark-based localization in robotics, as mentioned by \citet{thrun:prob_robo}.


\section{System Setup}
Before going into the detailed description of the solution, this section provides an overview of the system setup.

The implementation is written in Apple's new programming language \emph{Swift} (version 1.2), which is the successor of \emph{Objective-C}. The language is based on modern programming concepts, such as functional programming, tuples, generics, etc. Furthermore, Apple tries to remove unsafe code, such as not initialized variables, null-pointers, array overflows, etc.\ by defining the language's elements in a very concise and expressive syntax, and by adding new language constructs and types, such as the Optional type. The language compiles to native code by using LLVM compiler. Swift code can also call C and Objective-C code, which provides the possibility to use Swift in existing projects \citep{apple:swift}.

\begin{figure}[height=0.45\textheight]
	\input{figures/algorithmArchitecture}
	\caption{Depicts the solution's setup}
	\label{fig:algo_architecture}
\end{figure}

The system, depicted in figure~\ref{fig:algo_architecture}, consists of the \ac{PF}, its components, and the used \acsp{API}. Furthermore, the figure depicts the sensors used by the \acsp{API}. As explained in section~\ref{sec:fund_pf}, the algorithm uses two models, the \emph{Motion Model} and the \emph{Measurement Model}.

The Motion Model, described in section~\ref{sec:algo_motion_model}, is on one hand, responsible for tracking the user's motion by combining different sources. On the other hand, it samples from the motion model. To implement this, the Core Motion framework's pedometer and device motion component are used, which both use the smartphone's accelerometer, gyroscope and magnetometer, as described in chapter~\ref{chap:sensors}. Furthermore, the component uses the Core Location framework's compass, i.e.\ \texttt{CLHeading}, which uses the magnetometers data.

The Measurement Model, described in section~\ref{sec:algo_measurement_model}, implements the \acs{PF}'s importance factor calculation. For that it uses Core Location's \texttt{CLBeacon} component, which provides the \acs{RSS}-based distances estimations to the beacons. It uses the smartphone's \acf{BLE} module, to receive the beacons' \ac{BLE} signals. Furthermore, the Measurement Model uses the building's map.

The \acl{PF}, described in section~\ref{sec:algo_pf}, outputs the posterior, i.e.\ the position estimation represented by the particle set $\chi$. Furthermore, $\chi$ is passed to the function by its next iterative call.


\section{Motion Model}\label{sec:algo_motion_model}
The solution's motion model component is responsible for three tasks, firstly, for tracking the users motion, secondly, to determine if the user is stationary, and thirdly, to allow the \acs{PF} to sample from the motion model. The three tasks are prerequisites for the later explained \acs{PF} implementation.

\subsection{Motion Tracking}
As mentioned in chapter~\ref{chap:sensors}, \acl{CM} provides a component called \texttt{CMPedometer}, which estimates the traveled distance based on the steps a user has taken. In addition, \acl{CL} provides the smartphone's heading, based on the magnetic field, called \texttt{CLHeading}. Furthermore, \ac{CM}'s \texttt{CMDeviceMotion} provides the device attitude, which is calculated by using sensor fusion. The device's attitude can also be used to calculate the device's heading. By combining these three sources, the user's motion can be tracked, and a path can be constructed.

\subsubsection*{Heading}
As mentioned in chapter~\ref{chap:sensors}, the heading provided by \texttt{CLHeading} can be influenced by other magnetic fields than the earth's magnetic field, which may cause wrong values. \texttt{CMDeviceMotion} uses sensor fusion instead of relying only on the magnetometer's values. Thus, it is not influenced by other magnetic fields, but against the claims made by Apple's engineer \citet{apple:wwdc_2012_pham}, the values tend to drift away. Consequently, only relying on \texttt{CMDeviceMotion} is not sufficient. As a result, both sources are combined to determine the smartphone's heading.

Both frameworks do provide their values asynchronously. \acs{CL} calls its delegate method if a certain threshold of the heading's change, set to $1^\circ$, exceeds. Whereas, \acs{CM} calls its delegate periodically, every 0.02s. The high update rate would not be necessary for heading, but as mentioned earlier \texttt{CMDeviceMotion} also includes \texttt{userAccleration}, which is used for the stationary detection, as shown later.

The \texttt{CMDeviceMotion} heading, further denoted as $\theta_A$, measured at time $t$, is calculated from the rotation matrix, given by \texttt{CMDeviceMotion.attitude}, as explained in chapter~\ref{chap:sensors}. Due to its drift problem only the change between two values, denoted as $\Delta\theta_A$ is used. For the absolute orientation, \texttt{CLHeading.magneticHeading}, denoted as $\theta_C$, is used. Internally, the motion tracking uses the heading $\theta$, which takes the map's orientation $\theta_M$ into account. $\theta$ is calculated if \acs{CL}'s heading filter is exceeded and thus, a new magnetic heading is provided. Its not updated if \acs{CM} provides a new $theta_A$, due to its high update frequency, which would result in lots of useless updates of $\theta$ with very small change.

Listing~\ref{lst:motionModelHeadingCalculation} illustrates the used algorithm to calculate the internally used heading $\theta$ by combining the two heading sources for more robustness against influences of disturbing magnetic fields, and without the drift problem. Furthermore, table~\ref{tab:motionModelHeadingCalculationExample} provides a calculation example for better understanding.

\input{listings/motionModelHeadingCalculation}

\begin{table}
	\input{tables/motionModelHeadingCalculationExample}
	\caption{Example illustrating the calculation of the internal heading $\theta$, according to the algorithm depicted in listing~\ref{lst:motionModelHeadingCalculation}}
	\label{tab:motionModelHeadingCalculationExample}
\end{table}

\subsubsection*{Motion Path Construction}
As explained in chapter~\ref{chap:sensors}, \acs{CM} delivers every $\approx$~2.5s a new \texttt{CMPedometer} object with the estimation of the distance $d$, a user traveled. Besides the distance the object contains the start date $t_\text{start}$, which is the start date of the very first distance estimation. It is the same for all successive estimations. Additionally, it contains the end date $t_\text{end}$. Thus, the user walked an estimated distance $d$, beginning at $t_\text{start}$ and ending at $t_\text{end}$. During the walk, the user's direction $\theta$ changes several times at a certain point in time.

The user's motion path is stored as an array of motions $u$. A motion consists of $u = (\theta, d, t_\text{start}, t_\text{end})^T$, the orientation $\theta$ at the motions start date $t_\text{start}$, the distance $d$ in meters and the motions end date $t_\text{end}$. $t_\text{start}$ and $t_\text{end}$ are not really necessary for constructing the estimated motion path, but are necessary for the later explained motion integration.

To calculate the motion $u$ of a walked distance $d$, the distance is split up according to the timestamps corresponding to the smartphone's orientational, i.e.\ heading changes which occurred during the estimated distance. Constant velocity over the distance $d$ is assumed. Figure~\ref{fig:mm_path} illustrates the estimated path, a user traveled in 2-dimensional space. The individual distances $d_1, d_2$, estimated by \acs{CM}, are colored differently. By integrating the measured headings $\theta_0, \ldots, \theta_4$, the path is split up into motion $u_0, \ldots, u_5$.


\begin{figure}
	\input{figures/algorithmMotionModel_Path}
	\caption{Illustration of the motion path construction, by integrating heading $\theta_0, \ldots, \theta_4$ into the two estimated walked distances $d_1, d_2$. $u_0, \ldots, u_5$ depict the resulting motions.}
	\label{fig:mm_path}
\end{figure}


\subsection{Stationary Detection}\label{sec:algo_stationary}

\begin{figure}
	\input{figures/algorithmMotionModel_Stationary3Axis}
	\caption{Depicts the 3-axis \texttt{userAcceleration} provided by \acs{CM}'s \texttt{CMDeviceMotion} object. The user's two stationary phases are clearly depicted by the low amplitude. Remark: \texttt{userAcceleration} is without gravity.}
	\label{fig:mm_stationary_1}
\end{figure}

\begin{figure}
	\input{figures/algorithmMotionModel_StationaryNorm}
	\caption{Depicts the euclidian norm of the three-axis user acceleration shown in figure~\ref{fig:mm_stationary_1} and its simple moving average with a window of 1s which corresponds to 50 measurements.}
	\label{fig:mm_stationary_2}
\end{figure}

As mentioned in chapter~\ref{chap:sensors}, the \texttt{CMPedometer} component requires at the beginning, $\approx$~6~--~8~steps to deliver the first distance estimation. During a walk it continuously updates the estimation every $\approx$~2.5s. Due to the asynchronously incoming motion and beacon data, the filter cannot be run continuously, with a fixed time interval, because the later discussed measurements ,need to be integrated at the right position on the users walk. Thus, it is important to know if the user is currently walking, i.e.\ is stationary or not.

\citet{wang:wlan} proposes a system based on acceleration data to detect a user's steps by detecting the acceleration's zero-crossing; however, the system does not actually need to count the user's steps, which would require to much effort to get this information. Also,\citet{shanklin:embedded_sensors} use the user's acceleration to estimate the distance the user traveled, but they integrate the acceleration two times, to get the distance, with a preceding low pass filter. Thus, to detect if a user is stationary, one integration step of the user's acceleration would be sufficient, to get the user's velocity. Therefor, a projection of the measured acceleration data into the global coordinate system is required, as described in their work. Unfortunately, their solution worked very unreliable. According to \citet{wang:wlan}, integration of acceleration data for distance estimation works only in theory, but not reliably in indoor environments. Furthermore, their proposed projection is based on \texttt{CMDeviceMotion}'s \texttt{attitude} property, which suffers from the drift problem, as shown in section~\ref{sec:sensor_eval}.

But, \citet{shanklin:embedded_sensors} considered another solution for step detection, which uses the user's minimum acceleration. Steps are detected by using a threshold, that needs to be exceeded by the acceleration's Euclidian-Norm $\left\lVert a \right\rVert = \sqrt[2]{x^{2}+y^{2}+z^{2}}$. Figure~\ref{fig:mm_stationary_1} depicts a user's 3-axis acceleration during a walk with two, clearly visible, stops. The corresponding Euclidian-Norm is shown in figure~\ref{fig:mm_stationary_2}. For the actual detection of the stationary and not stationary state, a simple moving average where used. It is calculated over the last 50 values, i.e.\ by 50Hz the last second. If the threshold is exceeded $\left\lVert a \right\rVert > 0.1 ms^{-2}$, the user walks, if not, the user is stationary.

The solution's advantage is, that it is quite easy to implement and does not need a projection of the user's acceleration data. As a result, the later discussed integration of measurements, i.e.\ the importance factoring can be delayed when the user starts walking until \acs{CM} provides a distance estimation.
 
%\begin{equation} \label{eq:a}
%	\left\lVert a \right\rVert = \sqrt[2]{x^{2}+y^{2}+z^{2}}
%\end{equation}


\subsection{Sample Motion}\label{sec:algo_sample_motion}
As described in section~\ref{sec:fund_mcl}, \acs{MCL} has a \texttt{sample\_motion\_model} function to sample from the motion model, i.e.\ to apply a motion $u$ to a state hypothesis $x^{[m]}_{t-1}$ by taking the motion's uncertainties into account, as shown by equation~\ref{eq:sample_motion}.

\begin{equation}\label{eq:sample_motion}
	x^{[m]}_t = \left(
    \begin{array}{c}
      x_t\\
      y_t\\
      \theta_t
    \end{array}
  \right) = \left(\begin{array}{c} x_{t-1} + \cos(\theta_u + \theta_{\text{noise}})\cdot (d_u + d_\text{noise}) \\ y_{t-1} + \sin(\theta_u + \theta_{\text{noise}})\cdot (d_u + d_\text{noise}) \\ \theta_u + \theta_{\text{noise}}
    \end{array}
  \right)
\end{equation}

\noindent The new state hypothesis is denoted as $x^{[m]}_t$. A state is defined as $x^{[m]} = (x, y, \theta)^T$, where $x$ and $y$ denote the position in 2-dimensional space and $\theta$ the user's orientation. $d_u$ and $\theta_u$ are the distance and heading of motion $u$. The added noise $d_\text{noise}$ and $\theta_\text{noise}$ are the translational and rotational uncertainties, modeled as Gaussians. First, the uncertainties, determined during the sensor evaluation (sec.~\ref{sec:sensor_eval}) were used, but as usual, they do not fit best. By trying different values the following better fitting values where found.

\begin{equation}\label{eq:sigma_d}
	d_\text{noise} = NDF(\mu_\text{trans}, \sigma_\text{trans}) ,
	\quad \mu_\text{trans} = 0 ,
	 \quad \sigma_{\text{trans}} = \max(0.3, 0.3 \cdot d_u)
\end{equation}

\begin{equation}\label{eq:mu_d}
	\theta_\text{noise} = NDF(\mu_\text{rot}, \sigma_\text{rot}), \quad
	\mu_\text{rot} = 0 , \quad
	\sigma_\text{rot} = 20^{\circ}
\end{equation}

\noindent $d_\text{noise}$ depends on the motions distance $d_u$, shown in equation~\ref{eq:sigma_d}, whereas $\theta_\text{noise}$ uses constant parameters, shown in equation~\ref{eq:mu_d}. Due to the lack of a built-in algorithm to sample from a Gaussian distribution, the \texttt{sample\_normal\_distribution} algorithm proposed by \citet[p.~124]{thrun:prob_robo}, is used.


\section{Measurement Model}\label{sec:algo_measurement_model}
The measurement model component is responsible for the calculation of the importance factor $w^{[m]}$ for a given state hypothesis $x^{[m]}_t$ by taking the measurements $z_t$, its uncertainties, and the \texttt{map} into account. The \texttt{map} represents the environment, i.e.\ the building in form of a simple occupancy grid.

The algorithm depicted in listing~\ref{lst:measurementModelImportanceFactorCalculation} illustrates the calculation of the importance factor \texttt{weight} of one state hypothesis. If the state is out of the map's bounds or the position is not free, e.g.\ the map contains an obstacle at this position, the weight for the state is set to 0.0. If the states is valid, an importance factor $w$ for each of the measurements, i.e.\ for each distance estimation between the smartphone and the received beacons, is calculated.

\input{listings/measurementModelImportanceFactorCalculation}

To calculate $w$, the euclidian distance between the state hypothesis $x$, i.e.\ the particle and  the beacon $z_{i_\text{pos}}$ needs to be determined. It is the mean value $\mu_d$, required by the \emph{Probability Density Function} \texttt{PDF}. The \texttt{PDF}'s standard deviation $\sigma_d$ depends on the distance $z_{i_\text{dist}}$ between smartphone and the beacon. By trying different values, $\frac{1}{4}$ of $z_{i_\text{dist}}$ seems to fit best. The weight is than calculated by the \texttt{PDF}, shown in line 16. It is important to note, that just measurements with $z_{i_\text{dist}} < 5\text{m}$ are taken into account. Larger values are sorted out and not passed to the \texttt{measurement\_model} function, due to the fact, that estimated distances to a beacon larger than 5m are very unreliable, as shown in section~\ref{sec:beacon_eval}.

Finally, the weights $w$ of each measurement are multiplied with each other, which is the importance factor \texttt{weight} of this state hypothesis $x$, i.e.\ particle. The reason for adding the additional factor of 10 for each subsequent weight is, that otherwise the weights order of magnitude depends on the count of beacons. Of course, this only works, because usually $0.1 \leq w < 1$. Table~\ref{tab:measurementModelWeightFactorIllustration} illustrates the exponential decline of the \texttt{weight} by increasing beacon count $k$. For the illustration, all subsequent weights $w$ of their importance factor \texttt{weight}, do have the same value w~=~0.1. If the additional factor of 10 is being added, the beacon count is irrelevant for the weight's magnitude. Of course, during one run of the \acl{PF} the weight's magnitude is irrelevant, but to compare the sums of all importance factors over time, as used for the later discussed recovery from failure state, this is very important.

\begin{table}
	\input{tables/measurementModelWeightFactorIllustration}
	\caption{Illustrates the exponential decline of the importance factor \texttt{weight} by increasing beacon count $k$ if no additional factor is being added.}
	\label{tab:measurementModelWeightFactorIllustration}
\end{table}


\section{\acl{PF}}\label{sec:algo_pf}
The \acl{PF} is the solution's heart, which is responsible for the position estimation, by combining the before mentioned components. In this section first the \emph{initial generation of the particle set} is explained. Then two functions, named \texttt{integrateMotions} and \texttt{filter}, which are often called by the actual \ac{PF} implementation, are introduced. Afterwards the \ac{PF}'s implementation is explained. In the end the \emph{kidnapping problem's} solution to recover from failure state, is presented.


\subsection{Initial Particle Distribution}\label{sec:algo_initial}

\begin{figure}
\includegraphics[height=0.7\textwidth]{figures/algo_particle_generation}
\caption{Depicts the initial posterior belief, resp.\ the initial particle set. Particles are shown as red arrows, the beacons as blue filled circles. The large black circle is the $1\sigma$-ellipse.}
\label{fig:pf_initialDist}
\end{figure}

Before the \ac{PF} can start to continuously run, the initial posterior belief $\chi_0$, i.e.\ the particle set, needs to be generated. Tests showed, that a static particle set size of 200~particles is sufficient. As mentioned in chapter~\ref{chap:ibeacons}, each of the beacons can be uniquely identified, by combining the beacon's three identifiers. In addition, their position on the map, i.e.\ the local coordinate system, is known. Consequently, this information can be used to specifically distribute the initial particles around the received beacons. This is a huge advantage, instead of uniformly distributing them on the map's free space. For that reason, the implementation starts ranging for beacons and then wait's, until \ac{CL} reports the first ranged beacons. Then, the particles are distributed in a circle around the beacons, with radius of the estimated distance to that beacon. To take the distance estimation's uncertainty into account, each distance of a particle to its beacon gets added some Gaussian noise with $\mu = 0$ and $\sigma = 0.2 * d$, which depends on the estimated distance $d$. 

Additionally, the amount of particles being spread around a beacon, depends on the estimated proximity. 50\% of the particle set's size are spread around the beacon, which is most probably the closest one to the smartphone, according to its measured proximity. Around the second closest 25\%, the third 12.5\%, \ldots are being spread. The beacon which's proximity is furthest from the smartphone, gets the remaining particles.

The particles are generated with a random orientation $\theta$ where $\theta = [0, 2\pi)$. Figure~\ref{fig:pf_initialDist} shows a screenshot of the implemented iOS app, depicting the initial particle set. The particles are depicted as small red arrows, the beacons as blue filled circles. The black large circle shows the $1\sigma$~ellipse, which gets explained in section~\ref{sec:algo_locEstimation}.


\subsection{Motion Integration}
The \texttt{integrateMotion} function, shown in listing~\ref{lst:algorithmIntegrateMotion}, is a helper function to reduce the complexity of the later introduced algorithm. It gets passed the current particle set $\chi_{t-1}$ and the motion $u_t$ which it applies to each particle. Thus, it samples from the motion model, as discussed in section~\ref{sec:algo_sample_motion}. In the end, it returns the new particle set $\chi_t$.

\input{listings/algorithmIntegrateMotion}


\subsection{Filtering}
The \texttt{filter} function is another helper function, which is responsible for the \emph{importance factoring} and the \emph{resampling}, depicted in listing~\ref{lst:algorithmSimpleFilter}. It gets passed the particle set $\chi_{t-1}$, the measurements $z_t$ and the \texttt{map}. Then it determines the importance factor of each particle, as explained in section~\ref{sec:algo_measurement_model}. Afterwards, the resampling takes place, to transform the old distribution into the new posterior distribution, as described in section~\ref{sec:fund_pf}. The solution uses \emph{roulette wheel resampling}, which is a common resampling method based on \emph{independent sampling}, proposed by \citet[p. 108--111]{thrun:prob_robo}.

The shown implementation is the basic implementation which does not solve the kidnapping problem. To better understand the implementation, the \acs{PF}'s complexity was reduced, by first showing a simplified filter function, which neglects the recovery. Later in the chapter, the shown \texttt{filter} function is being enhanced, to explain the implemented recovery feature.

\input{listings/algorithmSimpleFilter}


\subsection{\acl{PF}}
After explaining all prerequisites, this section gives a detailed insight into the solution's \acl{PF} implementation. To reduce complexity, the \acs{PF}'s code is spit up into three logical parts, shown in listing~\ref{lst:algorithmParticleFilter_1}, \ref{lst:algorithmParticleFilter_2} and \ref{lst:algorithmParticleFilter_3}. Listing~\ref{lst:algorithmParticleFilter}, indicates their call order. It also contains several instance variables, which are accessible from the three parts.

The \texttt{particleFilter} function is continuously being called every $\approx$~1s, if new measurements are available. To remember, \ac{CL} calls its delegate to update the ranged beacons, whether it received a beacon's signal, or not. Consequently, when the function gets called, the latest measurements are always available and ready for processing. It gets passed the particle set $\chi_{t-1}$ and returns the set $\chi_t$.

The implementation uses a static particle set size of 200~particles. Tests showed, that 200~particles are sufficient and increasing the particle set does not really improve the estimation. The \texttt{particleFilter} function requires for one execution on average $\approx$~10ms. Thus, a dynamic particle set size to safe processing power is also not required.

\input{listings/algorithmParticleFilter}


\subsubsection*{Instance Variables}
As mentioned, the motions and measurements are delivered asynchronously by their frameworks. The motion model's implementation already reduces this problem by combining the two headings and the estimated walked distance into one motion. Thus, the \acs{PF}'s implementation has just to deal with two asynchronous sources. To overcome the asynchrony, the motions $u$ and measurements $z$ are buffered in two ascending ordered lists, the \texttt{u\_buffer} and \texttt{z\_buffer}, according their timestamps. The stored motions and measurements, are provided by the motion model and the measurement model, and defined as mentioned in section~\ref{sec:algo_motion_model} and section~\ref{sec:algo_measurement_model}. The instance variable $u_\text{latest}$ stores, as the name already implies, the latest motion that was calculated by the motion model. The \texttt{map} variable stores the environment's map in form of a occupancy grid, and the beacons' positions together with their unique identifiers.


\subsubsection*{\acl{PF} Function --- Part 1}
The algorithm's first part, shown in listing~\ref{lst:algorithmParticleFilter_1}, consists of basically one loop, which tries to integrate the buffered motions and to filter with the buffered measurements at the right point in time. The loop is executed as long as both buffers are not empty. The four if-cases are visualized in figure~\ref{fig:algo_pf_1}. In each example, the \texttt{u\_buffer} contains two motions $u_0, u_1$, and \texttt{z\_buffer} contains one measurement $z_0$. Each subfigure illustrates only the loop's first iteration. Thus, $u_1$ is not relevant during this iteration.

\begin{itemize}
\item{Case 1} (fig.\ \ref{fig:algo_pf_1_1}): If a motion ends before the measurement was taken ($u.{t_\text{end}} < z.t$), the motion is being integrated and $z$ is reserved for the next iteration.

\item{Case 2} (fig.\ \ref{fig:algo_pf_1_2}): If a measurement is taken exactly at the end of a motion ($u.{t_\text{end}} = z.t$), the motion is integrated. Afterwards, the set is filtered with the measurement.

\item{Case 3} (fig.\ \ref{fig:algo_pf_1_3}): If a measurement is taken during a motion ($u.{t_\text{start}} < z.t < u.{t_\text{end}}$), the motion needs to be split-up. The first sub motion is then being integrated into the particle set. Afterwards, it is filtered. The second sub motion is stored in the buffer at the first place, for being processed during the loop's next iteration.

\item{Case 4} (fig.\ \ref{fig:algo_pf_1_4}): If the measurement was taken before the motion ($u.{z.t < t_\text{start}}$ / \texttt{else}), the filter is executed without integrating the motion. The motion is kept back for the next iteration.

\end{itemize}

\noindent If one of the buffers is empty, the algorithm continues with the function's second part.

\input{listings/algorithmParticleFilter_1}

\begin{figure}
	\input{figures/algorithmParticleFilter_4Cases}
	\caption{Visually illustrates the four cases shown in listing~\ref{lst:algorithmParticleFilter_1}.}
	\label{fig:algo_pf_1}
\end{figure}


\subsubsection*{\acl{PF} Function --- Part 2}
If after executing the first part still motions are buffered, these motions are being integrated in this part. The reason for the remaining motions is, that \texttt{particleFilter} function is also being called if no measurements are received, as mentioned at the beginning. The reason why they can be integrated without current measurements is, they are definitely older than the not yet occurred measurements and thus there is no reason to keep them back.

After integrating a motion, the filter method is called. The filtering is executed without an actual measurement. Thus, only the map's occupancy grid is used to calculate the particle's importance factors. Consequently, particles that are moved onto an occupied position, are sorted out.

The purpose of not waiting for the next measurement to integrate the remaining motions is on one hand, that if the user is out of the beacons range, the motion is still being integrated; thus, the particles on the view are still moving according to the users motion. On the other hand the algorithms performance does not get worse, because \texttt{u\_buffer} never contains more motion's that need to be integrated, as in general.

\input{listings/algorithmParticleFilter_2}


\subsubsection*{\acl{PF} Function --- Part 3}
The first two part are important for integrating and filtering while a person is walking. But integrating measurements if a person is stationary can drastically improve the estimated location, as shown in chapter~\ref{chap:evaluation}. Thus, the received measurements are directly integrated, instead of buffering them, as shown in listing~\ref{lst:algorithmParticleFilter_3}. Furthermore, the user sees without a big delay, how the posterior changes by applying the measurements directly.

To be able to integrate them directly, the algorithm needs to be sure that the user is stationary. As mentioned in chapter~\ref{chap:sensors}, \acs{CL} has the problem of requiring 6~--~8~steps to recognize that a person is walking, i.e.\ until it delivers this information. Consequently, if a user is stationary and starts to walk, the measurements collected during the first steps need to be buffered until the motion data is available, otherwise the measurements would be integrated at the wrong position. The second important information is to know, that a person is now stationary, and no further motions are being delivered. Only then, the measurements can directly, without delay, being integrated.

The latter problem can be solved easily. Motions are delivered every $\approx$~2.5s. Thus, if the difference between now and the latest motion $u_\text{latest}$ is at least 2.7s, \acs{CM} does not deliver any new motion until the person starts to walk again. 2.7s was chosen, because the 2.5s are an approximate value. Besides that the \texttt{particleFilter} function is only called every $\approx$~1s, thus no additional delay is being created.

To be able to buffer the measurements during the first steps, when the user starts to walk again, the solution presented in section~\ref{sec:algo_stationary}, is used. Due to the solution's simple moving average over the last one second, it detects within $\approx$~1s if a user is stationary or not. Thus, if the user is not stationary, the motion integration is not executed, and the values are kept in the buffer.

Furthermore, tests showed, that by additionally integrating a small Gaussian distributed random motion with $\sigma = 0.3m$, helps to faster improve the location accuracy. If a person is stationary, the person's orientation does not matter. Consequently, also the motion's orientation $\theta$ is set to a uniformly distributed random value between $[0, 2 \cdot \pi)$. Using a random orientation has the advantage, that the particles can move in any direction.

\input{listings/algorithmParticleFilter_3}


\subsection{Recovery / Kidnapping Problem}\label{sec:algo_recovery}
The implemented recovery feature has two purposes. Firstly, it can happen, that all particles are moving out of bounds, and thus, are being filtered out. Secondly, if the particles are on a totally wrong position, which can happen if the user walked to soft, and thus the pedometer could not recognize the user's steps. It can also happen that the orientation does not fit the actual one; for instance, the user's phone points not straight forward during the walk.

To determine both situations, the before introduced \texttt{filter} function, shown in listing~\ref{lst:algorithmSimpleFilter}, was enhanced to support recovery. The enhanced version is shown in listing~\ref{lst:algorithmFilter}. During the resampling, the new particle set's \texttt{weightSum}~=~$\sum_{m = 0}^{M-1} w^{[m]}_t$, which is the sum of all drawn particles, during the resampling, is calculated.

If the $\texttt{weightSum} == 0$, $\chi_t$ is empty, thus all particles are moved out of bounds or their location is already occupied, and thus are being ignored. Consequently, a new random particle set is being generated. The particles are distributed as described in section~\ref{sec:algo_initial}, i.e\ the position estimation is restarted.

If the $\texttt{weightSum} > 0$, it gets normalized by the particle sets size. Then, it is stored in the \texttt{particleSums} array, which contains the latest three weightSums (line 24). When the \texttt{filter} function is called the next time, it first checks if the array already contains three weight sums, and if true, the average of the three weight sums is calculated (line 8). If the average drops below a certain threshold (set to 1.0), it is very likely, that the posterior's distribution does not match the user's true position, i.e.\ it is totally wrong. To recover from this state 20\% additional random particles where added to the particle set. The random particles are distributed as described in section~\ref{sec:algo_initial}. Due to the fact, that the particles are being added before the importance factoring and resampling, the added particles help to recover from the state, by getting higher importance factors, and thus they are drawn with a high probability.

Test of different random particle counts showed, that 20\% seems to bring the best result. Adding to less particles does not really help to recover, but adding to much results in a heavily jumping position estimation. The reason for taking the average over the last three weight sums is, that sometimes the weight sum drastically drops during one filter call. This happens for instance, if the measurements to the beacons where heavily influenced by an obstacle, and thus are totally wrong.

Figure~\ref{fig:algo_recovery} illustrates the recovery. The shown weight sums correspond to the two screenshots. During the experiment, the kidnapping problem was simulated by walking very smooth, approximately on the green path, from the lower lecture hall through the building's foyer, by passing the large obstacle at the left side, into the upper right lecture hall. After being their stationary for a few seconds, we returned, by passing the obstacle on the other side. During the walk, the iPhone's pedometer was not able to recognize any step. Both experiments, the one with recovery (fig.~\ref{fig:algo_recovery_withRecovery}) and the one without recovery (fig.~\ref{fig:algo_recovery_withoutRecovery}), are simulated by using the exact same recorded data, as input.

By testing different thresholds in different scenarios, a threshold of 1.0 seems fit best. In this example recovery was being executed the first time after $\approx$~90s. During the long break from 60s~--~90s the test user walked through the foyer, where no beacons with an estimated distance of less than 5m where ranged. Thus the \texttt{filter} function was not executed. After receiving measurements again, the average particle weight sum dropped below the threshold, and recovery took place. Thus, the particles jumped from the lower to the upper lecture hall. The same happened at $\approx$~155s on the way back. The third recovery is executed at $\approx$~184s, which caused a small jump next to the end position.

Without recovery, shown in figure~\ref{fig:algo_recovery_withoutRecovery}, the algorithm is not able to approximate the true location. After $\approx$~90s the weight sum drops to $\approx$~0, and stays there.


\input{listings/algorithmFilter}

\begin{figure}
	\input{figures/algorithmParticleFilter_Recovery}
	\caption {Depicts an example with and without the solution's recovery feature.}
	\label{fig:algo_recovery}
\end{figure}

\subsection{Location Estimation}\label{sec:algo_locEstimation}
To be able to display a concrete location estimation instead of a particle cloud, the \acs{PF}'s posterior belief, which is a multi-modal distribution, represented by the particle set $\chi_t$, needs to be converted into another distribution. As mentioned in chapter~\ref{chap:fundamentals}, uncertainties are typically modeled as Gaussians. Consequently, it is applicable to transform the \acs{PF}'s posterior into a two-dimensional gaussian $N(\mu, \Sigma)$, where the mean $\mu = (x, y)^T$ represents the concrete location. The location's uncertainty is expressed as the covariance matrix $\Sigma = \bigl(\begin{smallmatrix} \sigma_{x}^2&\sigma_{xy}\\ \sigma_{yx}&\sigma_{y}^2 \end{smallmatrix} \bigr)$.

The mean $\mu$, can either be $\chi$'s arithmetic mean or its weighted mean, by using the particle's importance factors as weights. Based on the estimated mean, the covariance $\Sigma$ can be determined. The Gaussian's parameters can then be used to visualize the user's location on the map in form of a $1\sigma\text{-ellipse}$. Thus the user's true location is with a probability of $\approx$~39\%, somewhere in the $1\sigma\text{-ellipse}$. Figure~\ref{fig:algo_sigellipse}, illustrates the transformation of the particle set $\chi$ into a gaussian, shown as $1\sigma\text{-ellipse}$.

\begin{figure}
	\includegraphics[width=0.7\textwidth]{figures/sigellipse}
	\caption{Depicts the transformation of $\chi$ into a gaussian, shown as $1\sigma\text{-ellipse}$, with a weighted mean.}
	\label{fig:algo_sigellipse}
\end{figure}
